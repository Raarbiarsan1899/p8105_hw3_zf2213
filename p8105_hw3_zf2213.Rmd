---
title: "P8105 Homework 3"
author: 'Zanis Fang, UID: ZF2213'
date: "10/4/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
 echo = TRUE,
 fig.width = 8,
 # fig proportion set to golden ratio
 fig.asp = 0.618,
 out.width = "100%",
 # set cache to reduce time
 cache = TRUE
 
)

# ggplot2 tibble dplyr tidyr ... etc.
library(tidyverse)

```

## Problem 1

### Data loading

```{r p1_loading_data_brfss}
# loading data
brfss <- p8105.datasets::brfss_smart2010 %>%
         # overall health topic
         filter(Topic == "Overall Health") %>% 
         # select necessary columns
         select(Year, Locationdesc, Response, Data_value) %>% 
         # seperate location into state and area
         separate(Locationdesc, c("state", "area"), sep = " - ") %>% 
         # recode response into factor variable
         mutate(Response = forcats::fct_relevel(Response,
                                                c("Excellent",
                                                  "Very good",
                                                  "Good",
                                                  "Fair",
                                                  "Poor"))) %>%
         # clean column names
         janitor::clean_names() %>% 
         # arrange dataset according to response from excellent to poor
         arrange(response)
 
```

*Q1. In 2002, which states were observed at 7 locations?*

```{r p1_q1_state_obs}
# get year 2002
brfss %>% filter(year == 2002) %>%
          # get unique area
          distinct(state, area) %>%
          # group by states
          group_by(state) %>%
          # number of observations for each state
          summarize(locations = n()) %>%
          # get states which observed at 7 locations
          filter(locations == 7) %>%
          # output table
          knitr::kable(caption = "States Observed", longtable = TRUE)
```

Connecticut, Florida, North Carolina are observed at 7 locations.


*Q2. Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.*

```{r p1_q2_spaghetti_plot}
# get distinct locations for each year each state
brfss %>% distinct(year, state, area) %>%
          # group according to year and state
          group_by(year, state) %>% 
          # count the number of observations for each year each state
          summarize(n_obs = n()) %>%
          # plot the spaghetti plot
          ggplot(aes(x = year, y = n_obs, color = state)) +
          geom_line() +
          geom_text(aes(label = ifelse(n_obs > 15, state, "")), show.legend = FALSE) +
          labs(
           title = "Unique locations across years for each state",
           x = "Years",
           y = "Number of Locations",
           caption = "Data from the brfss_smart2010"
          ) +
          viridis::scale_color_viridis(
           name = "state",
           discrete = TRUE
          ) +
          scale_x_continuous(breaks = c(2002:2010)) +
          theme(legend.position = "right", plot.title = element_text(hjust = 0.5))
          
```

Florida sharply increased the number of observations of more than 40 locations in year 2007 and 2010, and remained low during other years. New Jersey was on average the mostly observed state, around 20 observations each year started from 2005.


*Q3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.*

```{r p1_q3_table}

# filter out year 2002, 2006, 2010 in NY state with response "Excellent"
brfss %>% filter(year %in% c(2002, 2006, 2010), response == "Excellent", state == "NY") %>% 
          # group according to area
          group_by(area) %>% 
          # get mean and sd across three years
          summarize(mean = mean(data_value), sd = sd(data_value)) %>% 
          # make a readable table
          knitr::kable(digits = 2, caption = "Excellent proportions")
```

Since in Bronx, Erie and Monroe, there were only one observation out of three years, so sd are NAs. On average the New York County has most people responded excellent and least people responded excellent in Erie County.

*Q4. For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.*

```{r p1_q4_multi_spaghetti}

# grouping
brfss %>% group_by(year, state, response) %>% 
          # get mean for each combination
          summarize(mean = mean(data_value)) %>% 
          # plot multiple spaghetti plots
          ggplot(aes(x = year, y = mean, color = state)) + 
            geom_line() +
            # multi panels
            facet_grid(. ~ response) +
            labs(
              title = "Mean for each state, year across locations",
              x = "Years",
              y = "Mean of proportions",
              caption = "Data from the brfss_smart2010"
            ) +
          # make title in the middle
          theme(plot.title = element_text(hjust = 0.5),
                # make x tick 45 angle
                axis.text.x = element_text(angle = 45)
                )
          
```

The proportions of overall responses from high to low is "very good", "good", "excellent", "fair", "poor". It is worthy noting that he proportions of "excellent" are very slightly decreasing along the years. For each response there are some outliers in some states during some years.

## Problem 2

### Data loading

```{r p2_loading_data}
# load data
instacart <- p8105.datasets::instacart
```

This dataset has 1384617 entries. All the entries are the order information from registered customers. Each row describes an item ordered by a customer. For item, there are columns describe the name, department, aisle, and corresponding ID of the item. For order include the information about the customer ID, nth order from the customer, days since last order (frequency of order), time of the order in a week, time of the order in a day.

*Q1. How many aisles are there, and which aisles are the most items ordered from?*

```{r p2_q1_aisles}
# list how many distinct aisles
instacart %>% 
 count(aisle) %>%
 nrow()
 

# get number of aisles
instacart %>%
 group_by(aisle) %>%
 summarize(n_order = n()) %>%
 arrange(desc(n_order)) %>% 
 top_n(10)
 
```

There are 134 aisles. Fresh vegetables and fresh fruits aisles are ordered the most.

*Q2. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.*

```{r q2_aisle_plot, fig.width = 8, fig.asp = 1.618}

instacart %>%
 # group by aisle
 group_by(aisle) %>%
 # number of orders for each aisle
 summarize(n_order = n()) %>%
 # sort according to number of orders
 mutate(rank = rank(n_order)) %>% 
 # plot scatterplot
 ggplot(aes(x = reorder(aisle, rank), y = log(n_order))) +
   # plot points
   geom_bar(stat = "identity", aes(color = log(n_order))) +
   coord_flip() +
   # add labels and titles
   labs(
    title = "Number of orders for each aisle (order > 20000 are labelled)",
    x = "Percent rank of aisle order number",
    y = "Log transformed order numbers",
    caption = "Data from the instacart"
    ) +
   # make title in the middle
   theme(plot.title = element_text(hjust = 0.5),
         plot.margin = )
   
```

Vegetables, fruits, sparkling water, yogurt, cheese, milk, chips, pretzels, bread, soy are ordered most. All of them are food!

*Q3. Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”*

```{r q3_table}
instacart %>%
 # get the three aisle
 filter(aisle %in% c("baking ingredients",
                     "dog food care",
                     "packaged vegetables fruits")) %>%
 # group according to aisle and products
 group_by(aisle, product_name) %>%
 # number of orders for each item
 summarize(number_orders = n()) %>%
 # group by aisle
 group_by(aisle) %>%
 # get items with largest order number in each aisle
 filter(number_orders == max(number_orders)) %>%
 # output table
 knitr::kable(caption = "Most Popular Item Aisles",
              digits = 2)


```

Light brown sugar, snack sticks chicken & rice recipe dog treats, organic baby spinach are most ordered items in the corresponding three aisle. Organic baby spinach greatly outnumbers the other two.

*Q4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).*

```{r p2_q4_table}
instacart %>%
 # get the products
 filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
 # group by day of week and product name
 group_by(product_name, order_dow) %>%
 # get mean hours for each group
 summarize(mean_hour = mean(order_hour_of_day)) %>%
 # make human readable table
 spread(key = order_dow, value = mean_hour) %>%
 # output table
 knitr::kable(escape = FALSE)


```

People buy ice cream late from Monday to Thursday, early during weekends and Friday.
People buy apples around 2:00 pm in Wednesday and Sunday, but around noon for the rests.


## Problem 3

### Data loading

```{r}
ny_noaa <- p8105.datasets::ny_noaa %>%
 separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
 rename(prcp_01mm = prcp, snow_mm = snow, snwd_mm = snwd) %>%
 mutate(tmax = as.integer(tmax), tmin = as.integer(tmin))

ny_noaa %>% count(is.na(snow_mm))
ny_noaa %>% count(is.na(snwd_mm))
ny_noaa %>% count(is.na(prcp_01mm))
ny_noaa %>% count(is.na(tmax))
ny_noaa %>% count(is.na(tmin))
```

The dataset has 1384617 entries. 747 unique stations. There are 9 variables, station id, date of observation, precipitation (in 0.1 mm), snowfall (in mm), snow depth (in mm), max temperature (0.1 ºC), min temperature (0.1 ºC). Many stations only collect a subset of these data for some time. There are around 2.2 million (85.3%) entries without snowfall observations, 2.0 million (77.2%) entries without snow depth information, 2.5 million (94.4%) entries without precipitation data, 1.46 million (56.3%) without temperature data.
.


*Q1. For snowfall, what are the most commonly observed values? Why?*

```{r}

ny_noaa %>% 
 filter(!is.na(tmin), !is.na(snow_mm)) %>% 
 mutate(tmin_neg100 = tmin > -100) %>% 
 group_by(tmin_neg100) %>% 
 count(snow_mm == 0)
 
ny_noaa %>% 
 filter(!is.na(tmin), !is.na(snow_mm)) %>% 
 mutate(tmin_0 = tmin > 0) %>% 
 group_by(tmin_0) %>% 
 count(snow_mm == 0)

ny_noaa %>% 
 filter(!is.na(tmin), !is.na(snow_mm)) %>% 
 mutate(tmin_100 = tmin > 100) %>% 
 group_by(tmin_100) %>% 
 count(snow_mm == 0)
 
 
```



*Q2. Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?*

```{r}
ny_noaa %>% filter(month %in% c("01", "07")) %>%
 group_by(id, month) %>%
 summarize(mean_temp = mean(tmax, na.rm = TRUE)) %>%
 filter(!is.na(mean_temp)) %>% 
 ggplot(aes(y = mean_temp, x = id)) +
   geom_point() +
   geom_text(aes(label = ifelse(mean_temp < -60, id, "")), vjust = 1) +
   facet_grid(. ~ month)


```

*Q3. Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.*

```{r}
tmax_tmin <- ny_noaa %>% ggplot(aes(x = tmax, y = tmin)) + geom_density_2d()

snf <- ny_noaa %>% filter(snow_mm > 0, snow_mm < 100) %>% 
                   ggplot(aes(x = year, y = snow_mm)) +
                     geom_boxplot()
library(patchwork)
tmax_tmin + snf

```


