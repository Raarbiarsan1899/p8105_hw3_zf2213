---
title: "P8105 Homework 3"
author: 'Zanis Fang, UID: ZF2213'
date: "10/4/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# ggplot2 tibble dplyr tidyr ... etc.
library(tidyverse)

# install p8105.datasets if not already installed
if (!"p8105.datasets" %in% installed.packages()) {
	devtools::install_github("p8105/p8105.datasets")
}

# install ggrepel if not already installed
if (!"ggrepel" %in% installed.packages()) {
 devtools::install_github("slowkow/ggrepel")
}




```

## Problem 1

### Data loading

```{r}
brfss <- p8105.datasets::brfss_smart2010 %>%
         # overall health topic
         filter(Topic == "Overall Health") %>% 
         # select necessary columns
         select(Year, Locationdesc, Response, Data_value) %>% 
         # seperate location into state and area
         separate(Locationdesc, c("state", "area"), sep = " - ") %>% 
         # recode response into factor variable
         mutate(Response = forcats::fct_relevel(Response,
                                                c("Excellent",
                                                  "Very good",
                                                  "Good",
                                                  "Fair",
                                                  "Poor"))) %>%
         # clean column names
         janitor::clean_names() %>% 
         # arrange dataset according to response from excellent to poor
         arrange(response)
 
```

*Q1. In 2002, which states were observed at 7 locations?*

```{r}
# get year 2002
brfss %>% filter(year == 2002) %>%
          # get unique area
          distinct(state, area) %>%
          # group by states
          group_by(state) %>%
          # number of observations for each state
          summarize(locations = n()) %>%
          # get states which observed at 7 locations
          filter(locations == 7)
```

Connecticut, Florida, North Carolina are observed at 7 locations.


*Q2. Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.*

```{r}
# get distinct locations for each year each state
brfss %>% distinct(year, state, area) %>%
          # group according to year and state
          group_by(year, state) %>% 
          # count the number of observations for each year each state
          summarize(n_obs = n()) %>%
          # plot the spaghetti plot
          ggplot(aes(x = year, y = n_obs, color = state)) +
          geom_line() +
          viridis::scale_color_viridis(
           name = "state",
           discrete = TRUE
          ) +
          theme(legend.position = "none")
```

*Q3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.*

```{r}
# filter out year 2002, 2006, 2010 in NY state with response "Excellent"
brfss %>% filter(year %in% c(2002, 2006, 2010), response == "Excellent", state == "NY") %>% 
          # group according to area
          group_by(area) %>% 
          # get mean and sd across three years
          summarize(mean = mean(data_value), sd = sd(data_value)) %>% 
          # make a readable table
          knitr::kable()
```

Since in Bronx, Erie and Monroe, there were only one observation out of three years, so sd are NAs.

*Q4. For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.*

```{r}
brfss %>% group_by(year, state, response) %>% 
          summarize(mean = mean(data_value)) %>% 
          ggplot(aes(x = year, y = mean, color = state)) + 
            geom_line() +
            facet_grid(. ~ response)
```


## Problem 2

### Data loading

```{r}
# load data
instacart <- p8105.datasets::instacart



```

This dataset has 1384617 entries. All the entries are the order information from registered customers. Each row describes an item ordered by a customer. For item, there are columns describe the name, department, aisle, and corresponding ID of the item. For order include the information about the customer ID, nth order from the customer, days since last order (frequency of order), time of the order in a week, time of the order in a day.

*Q1. How many aisles are there, and which aisles are the most items ordered from?*

```{r}
# list how many distinct aisles
instacart %>% distinct(aisle) %>% nrow()

# 
instacart %>% group_by(aisle) %>% summarize(n_order = n()) %>% arrange(desc(n_order))
 


```

There are 134 aisles. Fresh vegetables and fresh fruits aisles are ordered the most.

*Q2. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.*

```{r}
instacart %>%
 group_by(aisle, department) %>%
 summarize(n_order = n()) %>%
 group_by(department) %>%
 arrange(n_order) %>% 
 mutate(id = 1:n()) %>%
 ggplot(aes(x = department, y = id, size = n_order)) +
   geom_point() +
   ggrepel::geom_text_repel(aes(label = aisle))



```


*Q3. Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”*



*Q4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).*


## Problem 3

### Data loading

```{r}
ny_noaa <- p8105.datasets::ny_noaa
```


